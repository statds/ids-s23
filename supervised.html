<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Data Science - 10&nbsp; Supervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./advanced.html" rel="next">
<link href="./visual.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Data Science</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./git.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Project Management with Git</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quarto.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducibile Data Science with Quarto</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python Refreshment</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pandas.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Manipulation with Pandas</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./geo.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Geospatial Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./descr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Descriptive Statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Statistical Tests and Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visual.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Visualization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./advanced.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Advanced Topics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nyccrash.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">NYC Crash Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">10.1</span>  Introduction</a></li>
  <li><a href="#classification-vs-regression" id="toc-classification-vs-regression" class="nav-link" data-scroll-target="#classification-vs-regression"><span class="toc-section-number">10.2</span>  Classification vs Regression</a>
  <ul class="collapse">
  <li><a href="#regression-metrics" id="toc-regression-metrics" class="nav-link" data-scroll-target="#regression-metrics"><span class="toc-section-number">10.2.1</span>  Regression metrics</a></li>
  <li><a href="#classification-metrics" id="toc-classification-metrics" class="nav-link" data-scroll-target="#classification-metrics"><span class="toc-section-number">10.2.2</span>  Classification metrics</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="toc-section-number">10.2.3</span>  Cross-validation</a></li>
  </ul></li>
  <li><a href="#support-vector-machines" id="toc-support-vector-machines" class="nav-link" data-scroll-target="#support-vector-machines"><span class="toc-section-number">10.3</span>  Support Vector Machines</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="toc-section-number">10.3.1</span>  Introduction</a></li>
  <li><a href="#package-that-need-to-install" id="toc-package-that-need-to-install" class="nav-link" data-scroll-target="#package-that-need-to-install"><span class="toc-section-number">10.3.2</span>  Package that need to install</a></li>
  <li><a href="#support-vector-classifier" id="toc-support-vector-classifier" class="nav-link" data-scroll-target="#support-vector-classifier"><span class="toc-section-number">10.3.3</span>  Support Vector Classifier</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">10.3.4</span>  Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="toc-section-number">10.3.5</span>  References</a></li>
  </ul></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="toc-section-number">10.4</span>  Decision Trees</a>
  <ul class="collapse">
  <li><a href="#introduction-2" id="toc-introduction-2" class="nav-link" data-scroll-target="#introduction-2"><span class="toc-section-number">10.4.1</span>  Introduction</a></li>
  <li><a href="#tree-algorithms" id="toc-tree-algorithms" class="nav-link" data-scroll-target="#tree-algorithms"><span class="toc-section-number">10.4.2</span>  Tree algorithms</a></li>
  <li><a href="#demo" id="toc-demo" class="nav-link" data-scroll-target="#demo"><span class="toc-section-number">10.4.3</span>  Demo</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="toc-section-number">10.4.4</span>  Conclusion</a></li>
  <li><a href="#references-1" id="toc-references-1" class="nav-link" data-scroll-target="#references-1"><span class="toc-section-number">10.4.5</span>  References</a></li>
  </ul></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><span class="toc-section-number">10.5</span>  Random forest</a>
  <ul class="collapse">
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm"><span class="toc-section-number">10.5.1</span>  Algorithm</a></li>
  </ul></li>
  <li><a href="#bagging-vs.-boosting" id="toc-bagging-vs.-boosting" class="nav-link" data-scroll-target="#bagging-vs.-boosting"><span class="toc-section-number">10.6</span>  Bagging vs.&nbsp;Boosting</a>
  <ul class="collapse">
  <li><a href="#introduction-3" id="toc-introduction-3" class="nav-link" data-scroll-target="#introduction-3"><span class="toc-section-number">10.6.1</span>  Introduction</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="toc-section-number">10.6.2</span>  Bagging</a></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="toc-section-number">10.6.3</span>  Boosting</a></li>
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison"><span class="toc-section-number">10.6.4</span>  Comparison</a></li>
  <li><a href="#references-2" id="toc-references-2" class="nav-link" data-scroll-target="#references-2"><span class="toc-section-number">10.6.5</span>  References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">10.1</span> Introduction</h2>
<p>Supervised learning uses labeled datasets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.</p>
<p>In contrast, unsupervised learning uses unlabeled data to discover patterns that help solve for clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set.</p>
</section>
<section id="classification-vs-regression" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="classification-vs-regression"><span class="header-section-number">10.2</span> Classification vs Regression</h2>
<ul>
<li>Classificaiton: outcome variable is categorical</li>
<li>Regression: outcome variable is continuous</li>
<li>Both problems can have many covariates (predictors/features)</li>
</ul>
<section id="regression-metrics" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="regression-metrics"><span class="header-section-number">10.2.1</span> Regression metrics</h3>
<ul>
<li>Mean squared error (MSE)</li>
<li>Mean absolute error (MAE)</li>
</ul>
</section>
<section id="classification-metrics" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="classification-metrics"><span class="header-section-number">10.2.2</span> Classification metrics</h3>
<section id="confusion-matrix" class="level4" data-number="10.2.2.1">
<h4 data-number="10.2.2.1" class="anchored" data-anchor-id="confusion-matrix"><span class="header-section-number">10.2.2.1</span> Confusion matrix</h4>
<p><a href="https://en.wikipedia.org/wiki/Confusion_matrix" class="uri">https://en.wikipedia.org/wiki/Confusion_matrix</a></p>
<p>Four entries in the confusion matrix:</p>
<ul>
<li>TP: number of true positives</li>
<li>FN: number of false negatives</li>
<li>FP: number of false positives</li>
<li>TN: number of true negatives</li>
</ul>
<p>Four rates from the confusion matrix with actual (row) margins:</p>
<ul>
<li>TPR: TP / (TP + FN). Also known as sensitivity.</li>
<li>FNR: TN / (TP + FN). Also known as miss rate.</li>
<li>FPR: FP / (FP + TN). Also known as false alarm, fall-out.</li>
<li>TNR: TN / (FP + TN). Also known as specificity.</li>
</ul>
<p>Note that TPR and FPR do not add up to one. Neither do FNR and FPR.</p>
<p>Four rates from the confusion matrix with predicted (column) margins:</p>
<ul>
<li>PPV: TP / (TP + FP). Also known as precision.</li>
<li>FDR: FP / (TP + FP).</li>
<li>FOR: FN / (FN + TN).</li>
<li>NPV: TN / (FN + TN).</li>
</ul>
</section>
<section id="measure-of-classification-performance" class="level4" data-number="10.2.2.2">
<h4 data-number="10.2.2.2" class="anchored" data-anchor-id="measure-of-classification-performance"><span class="header-section-number">10.2.2.2</span> Measure of classification performance</h4>
<p>Measures for a given confusion matrix:</p>
<ul>
<li>Accuracy: (TP + TN) / (P + N). The proportion of all corrected predictions. Not good for highly imbalanced data.</li>
<li>Recall (sensitivity/TPR): TP / (TP + FN). Intuitively, the ability of the classifier to find all the positive samples.</li>
<li>Precision: TP / (TP + FP). Intuitively, the ability of the classifier not to label as positive a sample that is negative.</li>
<li>F-beta score: Harmonic mean of precision and recall with <span class="math inline">\(\beta\)</span> chosen such that recall is considered <span class="math inline">\(\beta\)</span> times as important as precision, <span class="math display">\[
(1 + \beta^2) \frac{\text{precision} \cdot \text{recall}}
{\beta^2 \text{precision} + \text{recall}}
\]</span> See <a href="https://stats.stackexchange.com/questions/221997/why-f-beta-score-define-beta-like-that">stackexchange post</a> for the motivation of <span class="math inline">\(\beta^2\)</span>.</li>
</ul>
<p>When classification is obtained by dichotomizing a continuous score, the receiver operating characteristic (ROC) curve gives a graphical summary of the FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at all thresholds.</p>
<ul>
<li>Increasing from <span class="math inline">\((0, 0)\)</span> to <span class="math inline">\((1, 1)\)</span>.</li>
<li>Best classification passes <span class="math inline">\((0, 1)\)</span>.</li>
<li>Classification by random guess gives the 45-degree line.</li>
<li>Area between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.</li>
<li>Area under the curve (AUC) of ROC thus provides an important metric of classification results.</li>
</ul>
</section>
</section>
<section id="cross-validation" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">10.2.3</span> Cross-validation</h3>
<ul>
<li>Goal: strike a bias-variance tradeoff.</li>
<li>K-fold: hold out each fold as testing data.</li>
<li>Scores: minimized to train a model</li>
</ul>
<p>Cross-validation is an important measure to prevent over-fitting. Good in-sample performance does not necessarily mean good out-sample performance. A general work flow in model selection with cross-validation is as follows.</p>
<ul>
<li>Split the data into training and testing</li>
<li>For each candidate model <span class="math inline">\(m\)</span> (with possibly multiple tuning parameters)
<ul>
<li>Fit the model to the training data</li>
<li>Obtain the performance measure <span class="math inline">\(f(m)\)</span> on the testing data (e.g., CV score, MSE, loss, etc.)</li>
</ul></li>
<li>Choose the model <span class="math inline">\(m^* = \arg\max_m f(m)\)</span>.</li>
</ul>
<!-- ## Support Vector Machine -->
</section>
</section>
<section id="support-vector-machines" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="support-vector-machines"><span class="header-section-number">10.3</span> Support Vector Machines</h2>
<section id="introduction-1" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">10.3.1</span> Introduction</h3>
<p>Support Vector Machine (SVM) is a type of suppervised learning models that can be used to analyze classification and regression. In this section will develop the intuition behind support vector machines and provide some examples.</p>
</section>
<section id="package-that-need-to-install" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="package-that-need-to-install"><span class="header-section-number">10.3.2</span> Package that need to install</h3>
<p>Before we begin ensure that these this package are installed in your python</p>
<pre><code>pip install scikit-learn</code></pre>
<p><a href="https://scikit-learn.org/stable/">Scikit-learn</a> is a python package that provides efficient versions of a large number of common algorithms It constist of all type of machine learning model which is wildly known such as:</p>
<ul>
<li>Linear Regression</li>
<li>Logistic Regression</li>
<li>Decision Trees</li>
<li>Gaussian Process</li>
</ul>
<p>Furthermore, it also provide function that can be used anytime and use it on the provided machine learning algorithm. There are two type of functions:</p>
<ul>
<li>Avalable dataset functions such as Iris dataset <code>load_iris</code></li>
<li>Randomly generated datasets function such as <code>make_moon</code> , <code>make_circle</code> etc.</li>
</ul>
</section>
<section id="support-vector-classifier" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="support-vector-classifier"><span class="header-section-number">10.3.3</span> Support Vector Classifier</h3>
<p>Before we get into SVM , let us take a look at this simple classification problem. Consider a distinguishable datasets</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">220</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">50</span>, centers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                  random_state<span class="op">=</span> seed, cluster_std<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-2-output-1.png" width="569" height="411"></p>
</div>
</div>
<p>One of the solution we can do is to draw lines as a way to seperate these two classes.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> xfit(m,b):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">50</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> m<span class="op">*</span>t <span class="op">+</span> b</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">50</span>, centers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                  random_state<span class="op">=</span> <span class="dv">220</span>, cluster_std<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">50</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> xfit(<span class="dv">7</span>,<span class="op">-</span><span class="dv">5</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> xfit(<span class="dv">15</span>,<span class="dv">9</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> xfit(<span class="op">-</span><span class="dv">5</span>,<span class="op">-</span><span class="dv">4</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>ax.plot(t,y1,label <span class="op">=</span> <span class="st">'Line 1'</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>ax.plot(t,y2,label <span class="op">=</span> <span class="st">'Line 2'</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax.plot(t,y3,label <span class="op">=</span> <span class="st">'Line 3'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="dv">7</span>, <span class="dv">0</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>ax.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-3-output-1.png" width="569" height="416"></p>
</div>
</div>
<p>How do we find the best line that divide them both? In other word we need to find the optimal line or best decision boundary.</p>
<p>Lets import Support Vector Machine module for now to help us find the best line to classify the data set.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC <span class="co"># "Support vector classifier"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span><span class="fl">1E10</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For now lets not think about the purpose of C</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="dv">7</span>, <span class="dv">0</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> model.decision_function(xy).reshape(X_mesh.shape)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>ax.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>               levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span> ,<span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>               linestyles<span class="op">=</span> [<span class="st">'--'</span>,<span class="st">'-'</span>,<span class="st">'--'</span>])<span class="op">;</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>ax.scatter(model.support_vectors_[:, <span class="dv">0</span>],</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                   model.support_vectors_[:, <span class="dv">1</span>],</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>                   s<span class="op">=</span><span class="dv">300</span>, linewidth<span class="op">=</span><span class="dv">2</span>, facecolor <span class="op">=</span><span class="st">'none'</span>, edgecolor <span class="op">=</span> <span class="st">'black'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-4-output-1.png" width="569" height="416"></p>
</div>
</div>
<p>There is a name for this line. Is called <strong>margin</strong>, it is the shortest distance between the selected observation and the line. In this case we are using the largest margin to seperate the observation. We called it <strong>Maximal Margin Classifier</strong>.</p>
<p>The selected observation (circled points) are called <strong>Support Vectors</strong>. For simple explaination, it is the points that used to create the <strong>margin</strong>.</p>
<p>What if we have a weird observation as shown below? What happend if we try to use <strong>Maximal Margin Classifier</strong>? Lets add a point on an interesting location.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Addiing a point near yellow side and name it blue</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">50</span>, centers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                  random_state<span class="op">=</span> <span class="dv">220</span>, cluster_std<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> [<span class="dv">2</span>, <span class="op">-</span><span class="dv">4</span>]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([X,X_new])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>y_new <span class="op">=</span> np.array([<span class="dv">1</span>]).reshape(<span class="dv">1</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.append(y, [<span class="dv">0</span>], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.subplot()</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">51</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-5-output-1.png" width="569" height="411"></p>
</div>
</div>
<p>Using <strong>Maximum Margin Classifier</strong></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span><span class="fl">1E10</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="dv">7</span>, <span class="dv">0</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> model.decision_function(xy).reshape(X_mesh.shape)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>ax.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>               levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span> ,<span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>               linestyles<span class="op">=</span> [<span class="st">'--'</span>,<span class="st">'-'</span>,<span class="st">'--'</span>])<span class="op">;</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-6-output-1.png" width="569" height="416"></p>
</div>
</div>
<p>As you can see <strong>Maximal Margin Classifier</strong> might not be a useful in this case. We must make the margin that is not sensitve to outliers and allow a few misclassifications. So we need to implement <strong>Soft Margin</strong> to get a better prediction. This is where parameter C comes in.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># New fit with modifiying the C</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="dv">7</span>, <span class="dv">0</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> model.decision_function(xy).reshape(X_mesh.shape)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>ax.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>               levels<span class="op">=</span><span class="dv">0</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linestyles<span class="op">=</span> <span class="st">'-'</span>)<span class="op">;</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-7-output-1.png" width="569" height="416"></p>
</div>
</div>
<p>Increasing the parameter C will greatly influence the classification line location</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">100</span>, centers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>                  random_state<span class="op">=</span><span class="dv">0</span>, cluster_std<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>fig.subplots_adjust(left<span class="op">=</span><span class="fl">0.0625</span>, right<span class="op">=</span><span class="fl">0.95</span>, wspace<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> axi, C <span class="kw">in</span> <span class="bu">zip</span>(ax, [<span class="fl">100.0</span>, <span class="fl">0.1</span>]):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span>C).fit(X, y)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    axi.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">6</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    axi.set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">7</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    xlim <span class="op">=</span> axi.get_xlim()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    ylim <span class="op">=</span> axi.get_ylim()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a mesh grid</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> model.decision_function(xy).reshape(X_mesh.shape)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    axi.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>               levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, linestyles<span class="op">=</span>[<span class="st">'--'</span>,<span class="st">'-'</span>,<span class="st">'--'</span>])<span class="op">;</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    axi.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    axi.set_title(<span class="st">'C = </span><span class="sc">{0:.1f}</span><span class="st">'</span>.<span class="bu">format</span>(C), size<span class="op">=</span><span class="dv">14</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-8-output-1.png" width="649" height="434"></p>
</div>
</div>
<section id="support-vector-machine" class="level4" data-number="10.3.3.1">
<h4 data-number="10.3.3.1" class="anchored" data-anchor-id="support-vector-machine"><span class="header-section-number">10.3.3.1</span> Support Vector Machine</h4>
<p>Now we have some basic understanding on classifiying thing, lets take a look at the sample problem below.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_circles</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(<span class="dv">100</span>, factor<span class="op">=</span><span class="fl">.1</span>, noise<span class="op">=</span><span class="fl">.1</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-9-output-1.png" width="582" height="411"></p>
</div>
</div>
<p>If we apply a standard <strong>Support Vector Classifier</strong> the result will be like this.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>).fit(X, y)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> clf.decision_function(xy).reshape(X_mesh.shape)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>ax.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>               levels<span class="op">=</span><span class="dv">0</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linestyles<span class="op">=</span> <span class="st">'-'</span>)<span class="op">;</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-10-output-1.png" width="592" height="416"></p>
</div>
</div>
<p>This is not a good classifier. We need a way to make it better. Instead of just using the available data, let us try to convert a data to a better dimension space.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.exp(<span class="op">-</span>(X <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this case we will implement a kernel that will translate our data to a new diemension. This is one of the way to fit a nonlinear relationship with a linear classifier.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.subplot(projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>ax.scatter3D(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], r, c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#ax.view_init(elev=-90, azim=30)</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)<span class="op">;</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)<span class="op">;</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">'r'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-12-output-1.png" width="407" height="395"></p>
</div>
</div>
<p>Now you can see that it is seperated. We can apply the <strong>Support Vector Classifier</strong> to the dataset</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> r.reshape(<span class="dv">100</span>,<span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.concatenate((X,r),<span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>).fit(b, y)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>r_1 <span class="op">=</span> np.exp(<span class="op">-</span>(xy <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">1</span>))</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>r_1 <span class="op">=</span> r_1.reshape(<span class="dv">900</span>,<span class="dv">1</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>b_1 <span class="op">=</span> np.concatenate((xy,r_1),<span class="dv">1</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> clf.decision_function(b_1).reshape(X_mesh.shape)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>ax.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>                levels<span class="op">=</span><span class="dv">0</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linestyles<span class="op">=</span> <span class="st">'-'</span>)<span class="op">;</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-13-output-1.png" width="592" height="416"></p>
</div>
</div>
<p>Or you can just use SVC radial basis fucntion kernel to automatically create a decision boundary for you.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>).fit(X, y)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>Y_mesh, X_mesh <span class="op">=</span> np.meshgrid(y_grid,x_grid)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> clf.decision_function(xy).reshape(X_mesh.shape)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>ax.contour(X_mesh, Y_mesh, P, colors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>               levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, linestyles<span class="op">=</span>[<span class="st">'--'</span>, <span class="st">'-'</span>,<span class="st">'--'</span>])<span class="op">;</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>ax.scatter(clf.support_vectors_[:, <span class="dv">0</span>],</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>                   clf.support_vectors_[:, <span class="dv">1</span>],</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>                   s<span class="op">=</span><span class="dv">300</span>, linewidth<span class="op">=</span><span class="dv">2</span>, facecolor <span class="op">=</span><span class="st">'none'</span>, edgecolor <span class="op">=</span> <span class="st">'black'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-14-output-1.png" width="592" height="416"></p>
</div>
</div>
<p>As for summary, <strong>Support Vector Machine</strong> follow these steps:</p>
<ol type="1">
<li>Start with a data in low dimension.</li>
<li>Use kernel to move the data to a higher dimension.</li>
<li>Find a <strong>Support Vector Classifier</strong> that seperate the data into two groups.</li>
</ol>
</section>
<section id="kernel" class="level4" data-number="10.3.3.2">
<h4 data-number="10.3.3.2" class="anchored" data-anchor-id="kernel"><span class="header-section-number">10.3.3.2</span> Kernel</h4>
<p>Let talk more about the kernel. There are mutiple type of kernel. We will go through a few of them. Generaly, they call as a kernel trick or kernel method or kernel function. For simple explanation, these kernel can be view as a method on how we transform the data points into. It may need to transform to a higher dimension it may not.</p>
<ul>
<li><p>Linear Kernel The linear kernel is a kernel that uses the dot product of the input vectors to measure their similarity: <span class="math display">\[k(x,x')= (x\cdot x')\]</span></p></li>
<li><p>Polynomial Kernel</p>
<ul>
<li><p>For homogeneous case: <span class="math display">\[k(x,x')= (x\cdot x')^d\]</span> where if <span class="math inline">\(d = 1\)</span> it wil be act as linear kernel.</p></li>
<li><p>For inhomogeneous case: <span class="math display">\[k(x,x')= (x\cdot x' + r )^d\]</span> where r is a coefficient.</p></li>
</ul></li>
<li><p>Radial Basis Function Kernel (or rbf) is a well know kernel that can transform the data to a infinite dimension space.</p></li>
</ul>
<p>The function is known as:</p>
<p><span class="math inline">\(k(x,x') = \exp\left(-\gamma\left\Vert x-x' \right\Vert^2\right)\)</span></p>
<p><span class="math inline">\(\gamma &gt;0\)</span>. Sometimes parametrized using <span class="math inline">\(\gamma = \frac{1}{2\sigma^2}\)</span></p>
</section>
<section id="regression-problem" class="level4" data-number="10.3.3.3">
<h4 data-number="10.3.3.3" class="anchored" data-anchor-id="regression-problem"><span class="header-section-number">10.3.3.3</span> Regression Problem</h4>
<p>We will talk a little on Regression Problem and how it works on Support Vector Machine.</p>
<p>Lets consider a data output as shown below.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_regression(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">1</span>, noise<span class="op">=</span><span class="dv">10</span>, random_state <span class="op">=</span> <span class="dv">2220</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-15-output-1.png" width="577" height="411"></p>
</div>
</div>
<p>So how Support Vector Machine works for regression problem? Instead of giving some math formulas. Let do a fit and show the output of the graph.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">'linear'</span>, C <span class="op">=</span> <span class="dv">100</span>, epsilon <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_new)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_pred, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_pred <span class="op">+</span> model.epsilon, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_pred <span class="op">-</span> model.epsilon, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="supervised_files/figure-html/cell-16-output-1.png" width="577" height="411"></p>
</div>
</div>
<p>As you can see for regression problem Support Vector Machine for Regression or SVR create a two black lines as the decision boundary and the red line as the hyperplane. Our objective is to ensure points are within the boundary. The best fit line is the hyperplane that has a maximum number of points.</p>
<p>You can control the model by adjust the <code>C</code> value and <code>epsilon</code> value. <code>C</code> value change the slope of the line, lower the value will reduce the slope of the fit line. <code>epsilon</code> change the distance of the decision boundary, lower the <code>epsilon</code> reduce the distance of the dicision boundary.</p>
</section>
<section id="example-classification" class="level4" data-number="10.3.3.4">
<h4 data-number="10.3.3.4" class="anchored" data-anchor-id="example-classification"><span class="header-section-number">10.3.3.4</span> Example: Classification</h4>
<p>Let take a look at our NYC database. We would like to create a machine learning model with SVM.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>jan23 <span class="op">=</span> pd.read_csv(<span class="st">"data/nyc_crashes_202301_cleaned.csv"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>jan23.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>CRASH DATE</th>
      <th>CRASH TIME</th>
      <th>BOROUGH</th>
      <th>ZIP CODE</th>
      <th>LATITUDE</th>
      <th>LONGITUDE</th>
      <th>LOCATION</th>
      <th>ON STREET NAME</th>
      <th>CROSS STREET NAME</th>
      <th>OFF STREET NAME</th>
      <th>...</th>
      <th>Unnamed: 30</th>
      <th>Unnamed: 31</th>
      <th>Unnamed: 32</th>
      <th>Unnamed: 33</th>
      <th>Unnamed: 34</th>
      <th>Unnamed: 35</th>
      <th>Unnamed: 36</th>
      <th>Unnamed: 37</th>
      <th>Unnamed: 38</th>
      <th>Unnamed: 39</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1/1/23</td>
      <td>14:38</td>
      <td>BROOKLYN</td>
      <td>11211.0</td>
      <td>40.719094</td>
      <td>-73.946108</td>
      <td>(40.7190938,-73.9461082)</td>
      <td>BROOKLYN QUEENS EXPRESSWAY RAMP</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1/1/23</td>
      <td>8:04</td>
      <td>QUEENS</td>
      <td>11430.0</td>
      <td>40.659508</td>
      <td>-73.773687</td>
      <td>(40.6595077,-73.7736867)</td>
      <td>NASSAU EXPRESSWAY</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1/1/23</td>
      <td>18:05</td>
      <td>MANHATTAN</td>
      <td>10011.0</td>
      <td>40.742454</td>
      <td>-74.008686</td>
      <td>(40.7424543,-74.008686)</td>
      <td>10 AVENUE</td>
      <td>11 AVENUE</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1/1/23</td>
      <td>23:45</td>
      <td>QUEENS</td>
      <td>11103.0</td>
      <td>40.769737</td>
      <td>-73.912440</td>
      <td>(40.769737, -73.91244)</td>
      <td>ASTORIA BOULEVARD</td>
      <td>37 STREET</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1/1/23</td>
      <td>4:50</td>
      <td>BRONX</td>
      <td>10462.0</td>
      <td>40.830555</td>
      <td>-73.850720</td>
      <td>(40.830555, -73.85072)</td>
      <td>CASTLE HILL AVENUE</td>
      <td>EAST 177 STREET</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows  40 columns</p>
</div>
</div>
</div>
<p>Let us merge with <code>uszipcode</code> database to increase the number of input value to predict injury.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Calculate the sum</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>jan23[<span class="st">'sum'</span>] <span class="op">=</span> jan23[<span class="st">'NUMBER OF PERSONS INJURED'</span>] <span class="op">+</span> jan23[<span class="st">'NUMBER OF PEDESTRIANS INJURED'</span>]<span class="op">+</span> jan23[<span class="st">'NUMBER OF CYCLIST INJURED'</span>] <span class="op">+</span> jan23[<span class="st">'NUMBER OF MOTORIST INJURED'</span>]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> jan23.index:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> jan23[<span class="st">'sum'</span>][index] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        jan23.loc[index,[<span class="st">'injured'</span>]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        jan23.loc[index,[<span class="st">'injured'</span>]] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> uszipcode <span class="im">import</span> SearchEngine</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> SearchEngine()</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>resultlist <span class="op">=</span> []</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> jan23.index:</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    checkZip <span class="op">=</span> jan23[<span class="st">'ZIP CODE'</span>][index]</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.isnan(checkZip) <span class="op">==</span> <span class="va">False</span>:</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        zipcode <span class="op">=</span> <span class="bu">int</span>(checkZip)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> search.by_zipcode(zipcode)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        resultlist.append(result.to_dict())</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        resultlist.append({})</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>Zipcode_data <span class="op">=</span> pd.DataFrame.from_records(resultlist)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>merge <span class="op">=</span> pd.concat([jan23, Zipcode_data], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the repeated zipcode</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>merge <span class="op">=</span> merge.drop([<span class="st">'zipcode'</span>,<span class="st">'lat'</span>,<span class="st">'lng'</span>],axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>merge <span class="op">=</span> merge[merge[<span class="st">'population'</span>].notnull()]</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>Focus_data <span class="op">=</span> merge[[<span class="st">'radius_in_miles'</span>, <span class="st">'population'</span>, <span class="st">'population_density'</span>,</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="st">'land_area_in_sqmi'</span>, <span class="st">'water_area_in_sqmi'</span>, <span class="st">'housing_units'</span>,</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="st">'occupied_housing_units'</span>,<span class="st">'median_home_value'</span>,<span class="st">'median_household_income'</span>,<span class="st">'injured'</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These are the focus data that we will apply SVM to.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>Focus_data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>radius_in_miles</th>
      <th>population</th>
      <th>population_density</th>
      <th>land_area_in_sqmi</th>
      <th>water_area_in_sqmi</th>
      <th>housing_units</th>
      <th>occupied_housing_units</th>
      <th>median_home_value</th>
      <th>median_household_income</th>
      <th>injured</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.000000</td>
      <td>90117.0</td>
      <td>39209.0</td>
      <td>2.30</td>
      <td>0.07</td>
      <td>37180.0</td>
      <td>33489.0</td>
      <td>655500.0</td>
      <td>46848.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.909091</td>
      <td>50984.0</td>
      <td>77436.0</td>
      <td>0.66</td>
      <td>0.00</td>
      <td>33252.0</td>
      <td>30294.0</td>
      <td>914500.0</td>
      <td>104238.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.852273</td>
      <td>38780.0</td>
      <td>54537.0</td>
      <td>0.71</td>
      <td>0.00</td>
      <td>18518.0</td>
      <td>16890.0</td>
      <td>648900.0</td>
      <td>55129.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.000000</td>
      <td>75784.0</td>
      <td>51207.0</td>
      <td>1.48</td>
      <td>0.00</td>
      <td>31331.0</td>
      <td>29855.0</td>
      <td>271300.0</td>
      <td>45864.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.000000</td>
      <td>80018.0</td>
      <td>36934.0</td>
      <td>2.17</td>
      <td>0.05</td>
      <td>34885.0</td>
      <td>30601.0</td>
      <td>524100.0</td>
      <td>51725.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>To reduce the complexity, we will get 1000 sample from the dataset and import <code>train_test_split</code> to split up our data to measure the performance.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> Focus_data.sample(n<span class="op">=</span><span class="dv">1000</span>, random_state<span class="op">=</span><span class="dv">220</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Create X input</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> random_sample[[<span class="st">'radius_in_miles'</span>, <span class="st">'population'</span>, <span class="st">'population_density'</span>,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="st">'land_area_in_sqmi'</span>, <span class="st">'water_area_in_sqmi'</span>, <span class="st">'housing_units'</span>,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="st">'occupied_housing_units'</span>,<span class="st">'median_home_value'</span>,<span class="st">'median_household_income'</span>]].values</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Create Y for output</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>y  <span class="op">=</span> random_sample[<span class="st">'injured'</span>].values</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split </span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Apply SVM to our dataset and make a prediction on <code>X_test</code></p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>).fit(X_train, y_train)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Make prediction using X_test</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Check our accuracy of our model by importing <code>accuracy_score</code> from <code>sklearn.metrics</code></p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.69</code></pre>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10.3.4</span> Conclusion</h3>
<p>Support Vector Machines is one of the powerful tool mainly for classifications.</p>
<ul>
<li>Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.</li>
<li>Once the model is trained, the prediction phase is very fast.</li>
<li>Because they are affected only by points near the margin, they work well with high-dimensional dataeven data with more dimensions than samples, which is a challenging regime for other algorithms.</li>
<li>Their integration with kernel methods makes them very versatile, able to adapt to many types of data.</li>
</ul>
<p>However, SVMs have several disadvantages as well:</p>
<ul>
<li>The scaling with the number of samples <span class="math inline">\(N\)</span> is <span class="math inline">\(O[N^3]\)</span> at worst, or <span class="math inline">\(O[N^2]\)</span> for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.</li>
<li>The results are strongly dependent on a suitable choice for the softening parameter <span class="math inline">\(C\)</span>.This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.</li>
<li>The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the probability parameter of SVC), but this extra estimation is costly.</li>
</ul>
</section>
<section id="references" class="level3" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="references"><span class="header-section-number">10.3.5</span> References</h3>
<ul>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html">In-Depth: Support Vector Machines</a></li>
<li><a href="https://www.youtube.com/watch?v=efR1C6CvhmE">Support Vector Machines Video</a></li>
</ul>
<!-- ## Decision Tree -->
</section>
</section>
<section id="decision-trees" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">10.4</span> Decision Trees</h2>
<section id="introduction-2" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="introduction-2"><span class="header-section-number">10.4.1</span> Introduction</h3>
<p>Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.</p>
<p>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">1.png</figcaption><p></p>
</figure>
</div>
<p>picture source: <a href="https://scikit-learn.org/stable/modules/tree.html#" class="uri">https://scikit-learn.org/stable/modules/tree.html#</a></p>
<p>Here is an simple example of what the tree looks like.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">2.png</figcaption><p></p>
</figure>
</div>
<p>I will introduce the basics of the decision tree package in <code>scikit-learn</code> through this spam email classification example, using a simple mock dataset.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>mock_spam <span class="op">=</span> pd.read_csv(<span class="st">'data/mock_spam.csv'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>mock_spam</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>is_spam</th>
      <th>unknown_sender</th>
      <th>sales_words</th>
      <th>scam_words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Lets construct and visualize the model (tree version)</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>email_features <span class="op">=</span> mock_spam[[<span class="st">'unknown_sender'</span>, <span class="st">'sales_words'</span>, <span class="st">'scam_words'</span>]].values</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>is_spam <span class="op">=</span> mock_spam[[<span class="st">'is_spam'</span>]].values</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>) <span class="co"># Create a default classifier</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> clf.fit(email_features, is_spam)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>feat_names <span class="op">=</span> [<span class="st">'is unknown sender'</span>, <span class="st">'contain sales words'</span>, <span class="st">'contain scam words'</span>]</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'normal'</span>, <span class="st">'spam'</span>]</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(clf, out_file<span class="op">=</span><span class="va">None</span>, feature_names <span class="op">=</span> feat_names, </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>                                class_names<span class="op">=</span>class_names, filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>clf_graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>clf_graph</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<p><img src="supervised_files/figure-html/cell-24-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Both root and internal nodes have child nodes that branch out from them based on the value of a feature. For instance, the root node splits the unknown_sender feature space, and the threshold is 0.5. Its left subtree represents all the data with unknown_sender &lt;= 0.5, whereas its right subtree represents all the subset of data with unknown_sender &gt; 0.5. Each leaf node has an predicted value which will be used as the output from the decision tree. For example, the leftmost leaf node (left child of the root node) will lead to output is_spam = 0 (i.e.&nbsp;normal).</p>
<p>We can use this model to make some prediction.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>new_email_feat <span class="op">=</span> [[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], <span class="co"># Known sender, contains sales word, no scam word</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]] <span class="co"># Unknown sender, contains sales word, no scam word</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>clf.predict(new_email_feat) <span class="co"># expected result: 0 (normal), 1 (spam)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>array([0, 1])</code></pre>
</div>
</div>
<p>Given an input, the predicted outcome is obtained by traversing the decision tree. The traversal starts from the root node, and chooses left or right subtree based on the nodes splitting rule recursively, until it reaches a leaf node.</p>
<p>For the example input [1, 1, 0], its unknown_sender feature is 1, so we follow the right subtree based on the root nodes splitting rule. The next node splits on the scam_words feature, and since its value is 0, we follow the left subtree. The next node uses the sales_words feature, and its value is 1, so we should go down to the right subtree, where we reach a leaf node. Thus the predicted outcome is the value 1 (class spam).</p>
</section>
<section id="tree-algorithms" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="tree-algorithms"><span class="header-section-number">10.4.2</span> Tree algorithms</h3>
<p>As many other supervised learning approaches, the decision trees are constructed in a way that minimizes a chosen cost function. It is computationally infeasible to find the optimal decision tree that minimizes the cost function. Thus, a greedy approach known as recursive binary splitting is often used.</p>
<p>Package <code>scikit-learn</code> uses an optimized version of the CART algorithm; however, the <code>scikit-learn</code> implementation does not support categorical variables for now. Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting.</p>
<section id="cost-function" class="level4" data-number="10.4.2.1">
<h4 data-number="10.4.2.1" class="anchored" data-anchor-id="cost-function"><span class="header-section-number">10.4.2.1</span> Cost function</h4>
<p>Gini and entropy are classification criteria. Mean Squared Error (MSE or L2 error), Poisson deviance and Mean Absolute Error (MAE or L1 error) are Regression criteria. Here shows the mathematical formulations to get gini and entropy.</p>
<p>As we see from the above example, in a decision tree, each tree node <span class="math inline">\(m\)</span> is associated with a subset of the training data set. Assume there are <span class="math inline">\(n_m\)</span> data points associated with <span class="math inline">\(m\)</span>, and the class values of the data points are in the set <span class="math inline">\(Q_m\)</span>.</p>
<p>Further assume that there are K classes, and let <span class="math display">\[
p_{mk}=\frac{1}{n_m}\sum_{y\in Q_m}I(y=k) (k=1,...,K)
\]</span> represent the proportion of class <span class="math inline">\(k\)</span> observations in node <span class="math inline">\(m\)</span>. Then the cost functions (referred to as classification criteria in sklearn) available in sklearn are: * Gini: <span class="math display">\[
H(Q_m)=\sum_{k}p_{mk}(1-p_{mk})
\]</span> * Log loss or entropy: <span class="math display">\[
H(Q_m)=-\sum_{k}p_{mk}log(p_{mk})
\]</span></p>
<p>In <code>sklearn.tree.DecisionTreeClassifierhe</code>, the default criterion is gini. One advantage of using Gini impurity over entropy is that it can be faster to compute, since it involves only a simple sum of squares rather than logarithmic functions. Additionally, Gini impurity tends to be more robust to small changes in the data, while entropy can be sensitive to noise.</p>
</section>
<section id="how-to-choose-what-feature-and-threshold-to-split-on-at-each-node" class="level4" data-number="10.4.2.2">
<h4 data-number="10.4.2.2" class="anchored" data-anchor-id="how-to-choose-what-feature-and-threshold-to-split-on-at-each-node"><span class="header-section-number">10.4.2.2</span> How to choose what feature and threshold to split on at each node?</h4>
<p>The decision tree algorithm iterates over all possible features and thresholds and chooses the one that maximize purity or minimize impurity or maximize information gain.</p>
<p>Lets use the spam email example to calulate the impurity reduction.</p>
<p>The impurity reduction based on Gini is calculated as the difference between the Gini index of the parent node and the weighted average of the Gini of the child nodes. The split that results in the highest impurity reduction based on Gini is chosen as the best split.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>clf_graph</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<p><img src="supervised_files/figure-html/cell-26-output-1.svg" class="img-fluid"></p>
</div>
</div>
</section>
<section id="when-to-stop-splitting" class="level4" data-number="10.4.2.3">
<h4 data-number="10.4.2.3" class="anchored" data-anchor-id="when-to-stop-splitting"><span class="header-section-number">10.4.2.3</span> When to stop splitting?</h4>
<p>There are several stopping criteria that can be used to decide when to stop splitting in a decision tree algorithm. Here are some common ones:</p>
<p>When a node is 100% one class</p>
<p>Maximum depth: Stop splitting when the tree reaches a maximum depth, i.e., when the number of levels in the tree exceeds a predefined threshold.</p>
<p>Minimum number of samples: Stop splitting when the number of samples in a node falls below a certain threshold. This can help avoid overfitting by preventing the tree from making very specific rules for very few samples.</p>
<p>Minimum decrease in impurity: Stop splitting when the impurity measure (e.g., Gini impurity or entropy) does not decrease by a certain threshold after a split. This can help avoid overfitting by preventing the tree from making splits that do not significantly improve the purity of the resulting child nodes.</p>
<p>Maximum number of leaf nodes: Stop splitting when the number of leaf nodes reaches a predefined maximum.</p>
</section>
</section>
<section id="demo" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="demo"><span class="header-section-number">10.4.3</span> Demo</h3>
<section id="preparation" class="level4" data-number="10.4.3.1">
<h4 data-number="10.4.3.1" class="anchored" data-anchor-id="preparation"><span class="header-section-number">10.4.3.1</span> Preparation</h4>
<section id="step-1-install-scikit-learn" class="level5" data-number="10.4.3.1.1">
<h5 data-number="10.4.3.1.1" class="anchored" data-anchor-id="step-1-install-scikit-learn"><span class="header-section-number">10.4.3.1.1</span> Step 1: install scikit-learn</h5>
<p>Use pip</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> scikit-learn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Use Conda</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">-n</span> sklearn-env <span class="at">-c</span> conda-forge scikit-learn</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate sklearn-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-2-import-required-libraries" class="level5" data-number="10.4.3.1.2">
<h5 data-number="10.4.3.1.2" class="anchored" data-anchor-id="step-2-import-required-libraries"><span class="header-section-number">10.4.3.1.2</span> Step 2: Import Required Libraries</h5>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-3-preparing-the-data" class="level5" data-number="10.4.3.1.3">
<h5 data-number="10.4.3.1.3" class="anchored" data-anchor-id="step-3-preparing-the-data"><span class="header-section-number">10.4.3.1.3</span> Step 3: Preparing the Data</h5>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>NYC <span class="op">=</span> pd.read_csv(<span class="st">"data/merged.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># drop rows with missing data in some columns</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>NYC <span class="op">=</span> NYC.dropna(subset<span class="op">=</span>[<span class="st">'BOROUGH'</span>, <span class="st">'hour'</span>, <span class="st">'median_home_value'</span>, <span class="st">'occupied_housing_units'</span>])</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the features</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>nyc_subset <span class="op">=</span> NYC[[<span class="st">'BOROUGH'</span>, <span class="st">'hour'</span>, <span class="st">'median_home_value'</span>, <span class="st">'occupied_housing_units'</span>]].copy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One hot encode categorical features</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>nyc_encoded <span class="op">=</span> pd.get_dummies(nyc_subset, columns<span class="op">=</span>[<span class="st">'BOROUGH'</span>, <span class="st">'hour'</span>])</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>nyc_encoded</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>median_home_value</th>
      <th>occupied_housing_units</th>
      <th>BOROUGH_BRONX</th>
      <th>BOROUGH_BROOKLYN</th>
      <th>BOROUGH_MANHATTAN</th>
      <th>BOROUGH_QUEENS</th>
      <th>BOROUGH_STATEN ISLAND</th>
      <th>hour_0</th>
      <th>hour_1</th>
      <th>hour_2</th>
      <th>...</th>
      <th>hour_14</th>
      <th>hour_15</th>
      <th>hour_16</th>
      <th>hour_17</th>
      <th>hour_18</th>
      <th>hour_19</th>
      <th>hour_20</th>
      <th>hour_21</th>
      <th>hour_22</th>
      <th>hour_23</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>648900.0</td>
      <td>16890.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>271300.0</td>
      <td>29855.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>524100.0</td>
      <td>30601.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>654900.0</td>
      <td>10429.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>602400.0</td>
      <td>14199.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>7183</th>
      <td>445900.0</td>
      <td>12775.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7185</th>
      <td>445900.0</td>
      <td>12775.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7186</th>
      <td>397500.0</td>
      <td>22873.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7187</th>
      <td>655500.0</td>
      <td>33489.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7188</th>
      <td>426100.0</td>
      <td>26420.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>6663 rows  31 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train test split</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> <span class="op">\</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    train_test_split(nyc_encoded.values, NYC[[<span class="st">'injury'</span>]].values, test_size <span class="op">=</span> <span class="fl">0.20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="building-the-decision-tree-model" class="level4" data-number="10.4.3.2">
<h4 data-number="10.4.3.2" class="anchored" data-anchor-id="building-the-decision-tree-model"><span class="header-section-number">10.4.3.2</span> Building the Decision Tree Model</h4>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model and plot the tree (using default parameters)</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>injury_clf <span class="op">=</span> tree.DecisionTreeClassifier(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span><span class="st">'gini'</span>, splitter<span class="op">=</span><span class="st">'best'</span>, max_depth<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    min_samples_split<span class="op">=</span><span class="dv">2</span>, min_samples_leaf<span class="op">=</span><span class="dv">1</span>, min_weight_fraction_leaf<span class="op">=</span><span class="fl">0.0</span>, </span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="va">None</span>, max_leaf_nodes<span class="op">=</span><span class="va">None</span>, min_impurity_decrease<span class="op">=</span><span class="fl">0.0</span>, </span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    class_weight<span class="op">=</span><span class="va">None</span>, ccp_alpha<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>injury_clf <span class="op">=</span> injury_clf.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>injury_clf.tree_.node_count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>3599</code></pre>
</div>
</div>
<p>Arguments related to stopping criteria: * max_depth * min_samples_split * min_samples_leaf * min_weight_fraction_leaf * max_features * max_leaf_nodes * min_impurity_decrease</p>
<p>Other important arguments: * criterion: cost function to use * splitter: node splitting strategy * ccp_alpha: pruning parameter</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># define the hyperparameter grid for logistic regression</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">'criterion'</span>: [<span class="st">'gini'</span>, <span class="st">'entropy'</span>],</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">'max_depth'</span>: [<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>],</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">'min_impurity_decrease'</span>: [<span class="fl">1e-4</span>, <span class="fl">1e-3</span>, <span class="fl">1e-2</span>],</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">'ccp_alpha'</span>: [<span class="fl">0.0</span>, <span class="fl">1e-5</span>, <span class="fl">1e-4</span>, <span class="fl">1e-3</span>]}</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co"># perform cross-validation with GridSearchCV</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>tree_clf <span class="op">=</span> tree.DecisionTreeClassifier()</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(tree_clf, param_grid, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'f1'</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the GridSearchCV object to the training data</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co"># print the best hyperparameters found</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>grid_search.best_params_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>{'ccp_alpha': 1e-05,
 'criterion': 'gini',
 'max_depth': 20,
 'min_impurity_decrease': 0.0001}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use parameters from cross-validation to train another model</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>injury_clf2 <span class="op">=</span> tree.DecisionTreeClassifier(</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span><span class="st">'gini'</span>, splitter<span class="op">=</span><span class="st">'best'</span>, max_depth<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    min_samples_split<span class="op">=</span><span class="dv">2</span>, min_samples_leaf<span class="op">=</span><span class="dv">1</span>, min_weight_fraction_leaf<span class="op">=</span><span class="fl">0.0</span>, </span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="va">None</span>, max_leaf_nodes<span class="op">=</span><span class="va">None</span>, min_impurity_decrease<span class="op">=</span><span class="fl">0.0001</span>, </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    class_weight<span class="op">=</span><span class="va">None</span>, ccp_alpha<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>injury_clf2 <span class="op">=</span> injury_clf2.fit(X_train, y_train)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>injury_clf2.tree_.node_count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>933</code></pre>
</div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prune the tree more aggressively</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>injury_clf3 <span class="op">=</span> tree.DecisionTreeClassifier(</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span><span class="st">'gini'</span>, splitter<span class="op">=</span><span class="st">'best'</span>, max_depth<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    min_samples_split<span class="op">=</span><span class="dv">2</span>, min_samples_leaf<span class="op">=</span><span class="dv">1</span>, min_weight_fraction_leaf<span class="op">=</span><span class="fl">0.0</span>, </span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="va">None</span>, max_leaf_nodes<span class="op">=</span><span class="va">None</span>, min_impurity_decrease<span class="op">=</span><span class="fl">0.0001</span>, </span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    class_weight<span class="op">=</span><span class="va">None</span>, ccp_alpha<span class="op">=</span><span class="fl">8e-4</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>injury_clf3 <span class="op">=</span> injury_clf3.fit(X_train, y_train)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>injury_clf3.tree_.node_count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>15</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>injury_dot_data3 <span class="op">=</span> tree.export_graphviz(injury_clf3, out_file<span class="op">=</span><span class="va">None</span>, filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>injury_clf_graph <span class="op">=</span> graphviz.Source(injury_dot_data3)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>injury_clf_graph</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<p><img src="supervised_files/figure-html/cell-37-output-1.svg" class="img-fluid"></p>
</div>
</div>
</section>
<section id="evaluation" class="level4" data-number="10.4.3.3">
<h4 data-number="10.4.3.3" class="anchored" data-anchor-id="evaluation"><span class="header-section-number">10.4.3.3</span> Evaluation</h4>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># caculate the predicted values</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>clf_pred <span class="op">=</span> injury_clf.predict(X_test)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>clf2_pred <span class="op">=</span> injury_clf2.predict(X_test)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>clf3_pred <span class="op">=</span> injury_clf3.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the model</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, <span class="op">\</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>accuracy_score, precision_score, recall_score, f1_score, roc_auc_score</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>clf_cm <span class="op">=</span> confusion_matrix(y_test, clf_pred)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>clf2_cm <span class="op">=</span> confusion_matrix(y_test, clf2_pred)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>clf3_cm <span class="op">=</span> confusion_matrix(y_test, clf3_pred)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>clf_acc <span class="op">=</span> accuracy_score(y_test, clf_pred)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>clf2_acc <span class="op">=</span> accuracy_score(y_test, clf2_pred)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>clf3_acc <span class="op">=</span> accuracy_score(y_test, clf3_pred)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision</span></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>clf_precision <span class="op">=</span> precision_score(y_test, clf_pred)</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>clf2_precision <span class="op">=</span> precision_score(y_test, clf2_pred)</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>clf3_precision <span class="op">=</span> precision_score(y_test, clf3_pred)</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Recall</span></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>clf_recall <span class="op">=</span> recall_score(y_test, clf_pred)</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>clf2_recall <span class="op">=</span> recall_score(y_test, clf2_pred)</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>clf3_recall <span class="op">=</span> recall_score(y_test, clf3_pred)</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a><span class="co"># F1-score</span></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>clf_f1 <span class="op">=</span> f1_score(y_test, clf_pred)</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>clf2_f1 <span class="op">=</span> f1_score(y_test, clf2_pred)</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>clf3_f1 <span class="op">=</span> f1_score(y_test, clf3_pred)</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a><span class="co"># AUC</span></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>clf_auc <span class="op">=</span> roc_auc_score(y_test, clf_pred)</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a>clf2_auc <span class="op">=</span> roc_auc_score(y_test, clf2_pred)</span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>clf3_auc <span class="op">=</span> roc_auc_score(y_test, clf3_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Default parameter results:"</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion matrix:"</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf_cm)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, clf_acc)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, clf_precision)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, clf_recall)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1-score:"</span>, clf_f1)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AUC:"</span>, clf_auc)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cross-valiation parameter results:"</span>)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion matrix:"</span>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf2_cm)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, clf2_acc)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, clf2_precision)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, clf2_recall)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1-score:"</span>, clf2_f1)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AUC:"</span>, clf2_auc)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"More aggressive pruning results:"</span>)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion matrix:"</span>)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf3_cm)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, clf3_acc)</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, clf3_precision)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, clf3_recall)</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1-score:"</span>, clf3_f1)</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AUC:"</span>, clf3_auc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Default parameter results:
Confusion matrix:
[[579 233]
 [357 164]]
Accuracy: 0.5573893473368342
Precision: 0.41309823677581864
Recall: 0.31477927063339733
F1-score: 0.35729847494553374
AUC: 0.5139167289127577


Cross-valiation parameter results:
Confusion matrix:
[[626 186]
 [380 141]]
Accuracy: 0.5753938484621155
Precision: 0.43119266055045874
Recall: 0.2706333973128599
F1-score: 0.33254716981132076
AUC: 0.5207846789519964


More aggressive pruning results:
Confusion matrix:
[[752  60]
 [473  48]]
Accuracy: 0.6001500375093773
Precision: 0.4444444444444444
Recall: 0.09213051823416507
F1-score: 0.15262321144674085
AUC: 0.5091194463092007</code></pre>
</div>
</div>
</section>
</section>
<section id="conclusion-1" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">10.4.4</span> Conclusion</h3>
<p>In conclusion, decision trees are a widely used supervised learning algorithm for classification and regression tasks. They are easy to understand and interpret. The algorithm works by recursively splitting the dataset based on the attribute that provides the most information gain or the impurity reduction. The tree structure is built from the root node to the leaf nodes, where each node represents a decision based on a feature of the data.</p>
<p>One advantage of decision trees is their interpretability, which allows us to easily understand the decision-making process. They can also model complex problems with multiple outcomes. They are not affected by missing values or outliers.</p>
<p>However, decision trees can be prone to overfitting and may not perform well on complex datasets. They can also be sensitive to small variations in the training data and may require pruning to prevent overfitting. Random forest would be a better choice in this situation. Furthermore, decision trees may not perform well on imbalanced datasets, and their performance can be affected by the selection of splitting criteria.</p>
<p>Overall, decision trees are a useful and versatile tool in machine learning, but it is important to carefully consider their advantages and disadvantages before applying them to a specific problem.</p>
</section>
<section id="references-1" class="level3" data-number="10.4.5">
<h3 data-number="10.4.5" class="anchored" data-anchor-id="references-1"><span class="header-section-number">10.4.5</span> References</h3>
<p>
https://scikit-learn.org/stable/modules/tree.html#
</p><p>
</p><p>https://www.coursera.org/learn/advanced-learning-algorithms/home/week/4</p>
<!-- ## Random forest -->
</section>
</section>
<section id="random-forest" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">10.5</span> Random forest</h2>
<p>Random forest (RF) is a commonly-used ensemble machine learning algorithm. It is a bagging, also known as bootstrap aggregation, method, which combines the output of multiple decision trees to reach a single result.</p>
<ul>
<li>Regression: mean</li>
<li>Classification: majority vote</li>
</ul>
<section id="algorithm" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">10.5.1</span> Algorithm</h3>
<p>RF baggs on both data (rows) and features (columns).</p>
<ul>
<li>A random sample of the training data in a training set is selected with replacement (bootstrap)</li>
<li>A random subset of the features is selected as features (which ensures low correlation among the decision trees)</li>
<li>Hyperparameters
<ul>
<li>node size</li>
<li>number of trees</li>
<li>number of features</li>
</ul></li>
</ul>
<p>Use cross-valudation to select the hyperparameters.</p>
<p>Advantages:</p>
<ul>
<li>Reduced risk of overfitting since averaging of uncorrelated trees lowers overall variance and prediction error.</li>
<li>Provides flexibility in handeling missing data.</li>
<li>Easy to evaluate feature importance
<ul>
<li>Mean decrease in impurity (MDI): when a feature is excluded</li>
<li>Mean decrease accuracy: when the values of a feature is randomly permuted</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Computing intensive</li>
<li>Resource hungery</li>
<li>Interpretation</li>
</ul>
<!-- ## bagging versus boosting -->
</section>
</section>
<section id="bagging-vs.-boosting" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="bagging-vs.-boosting"><span class="header-section-number">10.6</span> Bagging vs.&nbsp;Boosting</h2>
<p><strong>By Nathan Nhan</strong></p>
<section id="introduction-3" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="introduction-3"><span class="header-section-number">10.6.1</span> Introduction</h3>
<p>Before we talk about Bagging and Boosting we first must talk about ensemble learning. Ensemble learning is a technique used in machine-learning where we have multiple models (often times called weak learners) that are trained to solve the same problem and then combined to obtain better results that they could individually. With this, we can obtain more accurate and more robust models for our data using this technique.</p>
<p>Bagging and boosting are two types of ensemble learning techniques. They decrease the variance of a single estimate as they combine multiple estimates from different models to create a model with higher stability. Additionally, ensemble learning techniques increase the stability of the final model by reducing faactors of error in our models such as unnecessary noise, bias, and variance that we might find hurts the accuracy of our model. Specifically:</p>
<ul>
<li>Bagging helps decrease the models variance and prevent over-fitting.</li>
<li>Boosting helps decrease the models bias.</li>
</ul>
</section>
<section id="bagging" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="bagging"><span class="header-section-number">10.6.2</span> Bagging</h3>
<p>Bagging, which stands for bootrap aggregation, is a ensemble learning algorithm designed to improve the stability and accuracy of algorithms used in statistical classification and regression. In Bagging, multiple homogenous algorithms are trained independently and combined afterward to determine the models average. It works like this:</p>
<ul>
<li>From the original dataset, multiple subsets are created, selecting observations with replacement.</li>
<li>On each of these subsets, a base learner (weak learner) is created for each</li>
<li>Next, all the independent models are run in parallel with one another</li>
<li>The final predictions are determined by combining the predictions from all the models.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Bagging.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Visualization on how the bagging algorithm works</figcaption><p></p>
</figure>
</div>
<p>The benefits of using bagging algorithms are that * Bagging algorithms reduce bias and variance errors. * Bagging algorithms can handle overfitting (when a model works with a training dataset but fails with the actual testing dataset). * Bagging can easily be implemented and produce more robust models.</p>
<section id="illustration" class="level4" data-number="10.6.2.1">
<h4 data-number="10.6.2.1" class="anchored" data-anchor-id="illustration"><span class="header-section-number">10.6.2.1</span> Illustration</h4>
<p>First we must load the dataset. For this topic I will be generating a madeup dataset shown below. It is recommended to make sure the dataset has no missing values as datasets with missing values leads to inconsistent results and poor model performance.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">0</span>)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a random dataset</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Age'</span>: [random.randint(<span class="dv">10</span>, <span class="dv">80</span>) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(length)],</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Weight'</span>: [random.randint(<span class="dv">110</span>, <span class="dv">250</span>) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(length)],</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Height'</span>: [random.randint(<span class="dv">55</span>, <span class="dv">77</span>) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(length)],</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Average BPM'</span>: [random.randint(<span class="dv">70</span>, <span class="dv">100</span>) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(length)],</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Amount of Surgeries'</span>: [random.randint(<span class="dv">0</span>, <span class="dv">3</span>) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(length)],</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Blood Pressure'</span>: [random.randint(<span class="dv">100</span>, <span class="dv">180</span>) <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(length)],</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'Healthy?'</span>] <span class="op">=</span> np.nan</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a random response variable displaying "1" for healthy and "0" for unhealthy</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row[<span class="st">'Blood Pressure'</span>] <span class="op">&lt;</span> <span class="dv">110</span> <span class="kw">and</span> row[<span class="st">'Average BPM'</span>] <span class="op">&gt;</span> <span class="dv">80</span>:</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>        df.at[index, <span class="st">'Healthy?'</span>] <span class="op">=</span> random.choice([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>        df.at[index, <span class="st">'Healthy?'</span>] <span class="op">=</span> random.choice([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Age</th>
      <th>Weight</th>
      <th>Height</th>
      <th>Average BPM</th>
      <th>Amount of Surgeries</th>
      <th>Blood Pressure</th>
      <th>Healthy?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>59</td>
      <td>125</td>
      <td>61</td>
      <td>92</td>
      <td>1</td>
      <td>177</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>63</td>
      <td>198</td>
      <td>71</td>
      <td>72</td>
      <td>0</td>
      <td>149</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15</td>
      <td>218</td>
      <td>72</td>
      <td>74</td>
      <td>0</td>
      <td>117</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>43</td>
      <td>144</td>
      <td>66</td>
      <td>96</td>
      <td>3</td>
      <td>132</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>75</td>
      <td>165</td>
      <td>69</td>
      <td>84</td>
      <td>3</td>
      <td>173</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>15</td>
      <td>113</td>
      <td>73</td>
      <td>74</td>
      <td>2</td>
      <td>147</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>11</td>
      <td>184</td>
      <td>63</td>
      <td>74</td>
      <td>3</td>
      <td>102</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>997</th>
      <td>38</td>
      <td>139</td>
      <td>73</td>
      <td>72</td>
      <td>1</td>
      <td>110</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>998</th>
      <td>51</td>
      <td>152</td>
      <td>68</td>
      <td>99</td>
      <td>0</td>
      <td>159</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>999</th>
      <td>18</td>
      <td>180</td>
      <td>56</td>
      <td>80</td>
      <td>1</td>
      <td>162</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>1000 rows  7 columns</p>
</div>
</div>
</div>
<p>Next, we need to specify the x and y variables where the x-variable will hold all the input columns containing numbers and y-variable will contain the output column.</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop([<span class="st">"Healthy?"</span>], axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'Healthy?'</span>]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Age  Weight  Height  Average BPM  Amount of Surgeries  Blood Pressure
0     59     125      61           92                    1             177
1     63     198      71           72                    0             149
2     15     218      72           74                    0             117
3     43     144      66           96                    3             132
4     75     165      69           84                    3             173
..   ...     ...     ...          ...                  ...             ...
995   15     113      73           74                    2             147
996   11     184      63           74                    3             102
997   38     139      73           72                    1             110
998   51     152      68           99                    0             159
999   18     180      56           80                    1             162

[1000 rows x 6 columns]</code></pre>
</div>
</div>
<p>Next we must scale our data. Dataset scaling is transforming the dataset to fit within a specific range. This ensures that no data point is left out during model training. In the example giving we will use the <code>StandardScaler</code> method to scale our dataset.</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_scaled)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Age  Weight  Height  Average BPM  Amount of Surgeries  Blood Pressure
0     59     125      61           92                    1             177
1     63     198      71           72                    0             149
2     15     218      72           74                    0             117
3     43     144      66           96                    3             132
4     75     165      69           84                    3             173
..   ...     ...     ...          ...                  ...             ...
995   15     113      73           74                    2             147
996   11     184      63           74                    3             102
997   38     139      73           72                    1             110
998   51     152      68           99                    0             159
999   18     180      56           80                    1             162

[1000 rows x 6 columns]
[[ 0.71876116 -1.33938121 -0.78473753  0.77849777 -0.44769866  1.59373784]
 [ 0.91291599  0.44444037  0.71600359 -1.46533498 -1.3613694   0.39947944]
 [-1.41694191  0.93315861  0.8660777  -1.2409517  -1.3613694  -0.96538731]
 ...
 [-0.30055167 -0.99727844  1.01615181 -1.46533498 -0.44769866 -1.26395191]
 [ 0.33045151 -0.67961158  0.26578125  1.56383923 -1.3613694   0.82600029]
 [-1.27132579  0.00459395 -1.53510809 -0.56780188 -0.44769866  0.95395655]]</code></pre>
</div>
</div>
<p>After scaling the dataset, we can split it. We will split the scaled dataset into two subsets: training and testing. To split the dataset, we will use the train_test_split method. We will be using the default splitting ratio for the <code>train_test_split</code> method which means that 80% of the data will be the training set and 20% the testing set.</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_scaled, y, stratify<span class="op">=</span>y, random_state<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have split our data, we can now perform our classification. The <code>BaggingClassifier</code> classifier will perform all the bagging steps and build an optimized model based on our data. The <code>BaggingClassifier</code> will fit the weak/base learners on the randomly sampled subsets. The listed parameters are as follows:</p>
<ul>
<li><code>estimator</code> - this parameter takes the algoithm we want to use, in this example we use the <code>DecisionTreeClassifier</code> for our weak learners.</li>
<li><code>n_estimators</code> - this parameter takes the amount of weak learners we want to use.</li>
<li><code>max_samples</code> - this parameter represents The maximum number of data that is sampled from the training set. We use 80% of the training dataset for resampling.</li>
<li><code>bootstrap</code> - this parameter allows for resampling of the training dataset without replacement when set to <code>True</code>.</li>
<li><code>oob_score</code> - this parameter is used to compute the models accuracy score after training.</li>
<li><code>random_state</code> - Seed used by the random number generator so we can reproduce out results</li>
</ul>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>bag_model <span class="op">=</span> BaggingClassifier(</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>estimator<span class="op">=</span>DecisionTreeClassifier(), <span class="co"># DecisionTreeClassifier() if estimator is not defined</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>n_estimators<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>oob_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>random_state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>bag_model.fit(X_train, y_train)</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>AccScore <span class="op">=</span> bag_model.oob_score_</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy Score for Bagging Classifier: "</span> <span class="op">+</span> <span class="bu">str</span>(AccScore))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy Score for Bagging Classifier: 0.8853333333333333</code></pre>
</div>
</div>
<p>We can find if overfitting occurs when we get a lower accuracy when using the testing dataset. We can find this by running the following code:</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> bag_model.score(X_test, y_test) <span class="op">&lt;</span> AccScore:</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Overfitting has occurred since the testing dataset has a lower accuracy than the training dataset'</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"No overfitting has occurred"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overfitting has occurred since the testing dataset has a lower accuracy than the training dataset</code></pre>
</div>
</div>
<p>Comparing this to the non-bagging algorithm like K-fold cross-validation:</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(DecisionTreeClassifier(), X, y, cv<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy Score for Bagging Classifier: "</span> <span class="op">+</span> <span class="bu">str</span>(scores.mean())) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy Score for Bagging Classifier: 0.78</code></pre>
</div>
</div>
</section>
<section id="example-random-forest-classifier" class="level4" data-number="10.6.2.2">
<h4 data-number="10.6.2.2" class="anchored" data-anchor-id="example-random-forest-classifier"><span class="header-section-number">10.6.2.2</span> Example: Random Forest Classifier</h4>
<p>The Random Forest Classifier algorithm is a typical example of a bagging algorithm. Random Forests uses bagging underneath to sample the dataset with replacement randomly. Random Forests samples not only data rows but also columns. It also follows the bagging steps to produce an aggregated final model.</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>n_estimators<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>bootstrap <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>oob_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>random_state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> clf.fit(X, y)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy Score for Random Forest Regression Algoritm Classifier: "</span> <span class="op">+</span> <span class="bu">str</span>(clf.oob_score_)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy Score for Random Forest Regression Algoritm Classifier: 0.869</code></pre>
</div>
</div>
</section>
</section>
<section id="boosting" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="boosting"><span class="header-section-number">10.6.3</span> Boosting</h3>
<p>The Boosting algorithm is a type of ensemble learning algorithm that works by training weak models sequentially. First, a model is built from the training data. Then an additional model is built which tries to correct the errors present in the first model. When an input is misclassified by a model, its weight is increased so that next model is more likely to classify it correctly. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of n models are added.</p>
<p>It works step-by-step like this:</p>
<ul>
<li>A subset is created from the original dataset, where initially all the data points are given equal weights.</li>
<li>A base model is created on this subset which is used to make predictions on the whole dataset.</li>
<li>Errors are calculated by comparing the actual values vs.&nbsp;the predicted values. The observations that are incorrectly predicted, are given higher weights.</li>
<li>An additional model is created and predictions are made on the dataset, trying to correct the errors of the previous model</li>
<li>The cycle will repeat until the maximum number of n models are added.</li>
<li>The final model (strong model) is the weighted mean of all the models (weak model).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Boosting.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Visualization on how the boosting algorithm works</figcaption><p></p>
</figure>
</div>
<section id="example-adaboost" class="level4" data-number="10.6.3.1">
<h4 data-number="10.6.3.1" class="anchored" data-anchor-id="example-adaboost"><span class="header-section-number">10.6.3.1</span> Example: AdaBoost</h4>
<p>The AdaBoost algorithm, which is short for Adaptive Boosting, was one of the first boosting methods that saw an increase in accuracy and speed performance of models. AdaBoost focuses on enhnacement in performance in areas where the first iteration of the model fails.</p>
<p>We can see an implmentation of this algorithm below:</p>
<p>The <code>AdaBoostClassifier</code> method has some of the following parameters:</p>
<ul>
<li><code>estimator</code> - this parameter takes the algoithm we want to use, in this example we use the <code>DecisionTreeClassifier</code> for our weak learners.</li>
<li><code>n_estimators</code> - this parameter takes the amount of weak learners we want to use.</li>
<li><code>learning_rate</code> - this parameter learning rate reduces the contribution of the classifier by this value. It has a default value of 1.</li>
<li><code>random_state</code> - Seed used by the random number generator so we can reproduce out results.</li>
</ul>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>adaboost <span class="op">=</span> AdaBoostClassifier(</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    estimator <span class="op">=</span> DecisionTreeClassifier(),</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    n_estimators <span class="op">=</span> <span class="dv">100</span>, </span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>adaboost.fit(X_train, y_train)</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> adaboost.score(X_test, y_test)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy Score for Adaboost Classifier: "</span> <span class="op">+</span> <span class="bu">str</span>(score)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy Score for Adaboost Classifier: 0.76</code></pre>
</div>
</div>
</section>
<section id="example-xgboost" class="level4" data-number="10.6.3.2">
<h4 data-number="10.6.3.2" class="anchored" data-anchor-id="example-xgboost"><span class="header-section-number">10.6.3.2</span> Example: XGBoost</h4>
<p><code>XGBoost</code> is widely considered as one of the most important boosting methods for its advantages over other boosting algorithms.</p>
<p>Other boosting algorithms typically use gradiant descent to find the minima of the <code>n</code> features mapped in <code>n</code> dimensional space. However, <code>XGBoost</code> instead uses the a mathematical technique called the NewtonRaphson method which uses the second derivative of the function which provides curvature information in contract to algorithms using gradiant descent which only use the first derivative.</p>
<p><code>XGBoost</code> has its own python library called <code>xgboost</code> just to itself for which we wrote the example code below.</p>
<p>The <code>XGBClassifer</code> contains some of the following parameters:</p>
<ul>
<li><code>learning_rate</code> - this parameter takes the amount of weak learners we want to use.</li>
<li><code>random_state</code> - Random number seed.</li>
<li><code>importance_type</code> - The feature to focus on; either gain, weight, cover, total_gain or total_cover.</li>
</ul>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>xgboost <span class="op">=</span> XGBClassifier(</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    n_estimators <span class="op">=</span> <span class="dv">1000</span>, </span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>xgboost.fit(X_train, y_train)</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>score_xgb <span class="op">=</span> xgboost.score(X_test,y_test)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy Score for XGBoost Classifier: "</span> <span class="op">+</span> <span class="bu">str</span>(score_xgb)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy Score for XGBoost Classifier: 0.836</code></pre>
</div>
</div>
</section>
</section>
<section id="comparison" class="level3" data-number="10.6.4">
<h3 data-number="10.6.4" class="anchored" data-anchor-id="comparison"><span class="header-section-number">10.6.4</span> Comparison</h3>
<p>Now with all that being said, which should you use? Bagging or Boosting?</p>
<p>Both Bagging and Boosting combine several estimates from different models so both will turn out a model with higher stability.</p>
<p>If you find that the problem when using a single model gets a high error, bagging will rarely be better but on the other hand boosting will generate a combined model with lower errors.</p>
<p>Conversely, if your single model is over-fitting then typically bagging is the best option and boosting will not help for over-fitting. Therefore, bagging iw what you want.</p>
<section id="similarites" class="level4" data-number="10.6.4.1">
<h4 data-number="10.6.4.1" class="anchored" data-anchor-id="similarites"><span class="header-section-number">10.6.4.1</span> Similarites</h4>
<p>Below is a set of similarites between Bagging and Boosting:</p>
<ul>
<li><p>Both are ensemble learning methods to get N models all from one individual model</p></li>
<li><p>Both use random sampling to generate several random subsets</p></li>
<li><p>Both make the final decision by averaging and combining the N learners (or taking the majority of them i.e Majority Voting).</p></li>
<li><p>Both reduce variance and provide higher data stability than one individual model would.</p></li>
</ul>
</section>
<section id="differences" class="level4" data-number="10.6.4.2">
<h4 data-number="10.6.4.2" class="anchored" data-anchor-id="differences"><span class="header-section-number">10.6.4.2</span> Differences:</h4>
<p>Below is a set of differences between Bagging and Boosting:</p>
<ul>
<li>Bagging combines predictions belonging to the same type while boosting is a way of combining predictions that belong to different types</li>
<li>Bagging decreases variance while boosting decreases bias. If the classifier is unstable (high variance), then we using Bagging. If the classifier is stable and simple (high bias), then we should use Boosting.</li>
<li>Bagging each model receives equal weight where in Boosting each model is weighted based on their performance.</li>
<li>Bagging has models run in parallel, independent of one another while Boosting has them run sequentially so each model is dependent on the previous</li>
<li>In Bagging different training data subsets are randomly generated with replacement segemnts of the original training dataset. In Boosting each new subsets contains the elements that were misclassified by previous models.</li>
<li>Bagging attempts to solve over-fitting problem while Boosting does not.</li>
</ul>
</section>
<section id="conclusion-2" class="level4" data-number="10.6.4.3">
<h4 data-number="10.6.4.3" class="anchored" data-anchor-id="conclusion-2"><span class="header-section-number">10.6.4.3</span> Conclusion:</h4>
<ul>
<li>We have explained what an ensemble method is and how Bagging and Boosting algorithms function.</li>
<li>We have demonstrated the differences and similaries between the two ensemble methods: Bagging and Boosting.</li>
<li>We showed how to implement both Bagging and Boosting algorithms into Python</li>
</ul>
</section>
</section>
<section id="references-2" class="level3" data-number="10.6.5">
<h3 data-number="10.6.5" class="anchored" data-anchor-id="references-2"><span class="header-section-number">10.6.5</span> References</h3>
<p><a href="https://www.kaggle.com/code/prashant111/bagging-vs-boostingaggingClassifier">Kaggle: Bagging vs.&nbsp;Boosting</a></p>
<p><a href="https://scikit-learn.org/stable/modules/ensemble.html">Scikit.learn</a></p>
<p><a href="https://www.section.io/engineering-education/implementing-bagging-algorithms-in-python/#how-bagging-works">Bagging algorithms in Python</a></p>
<p><a href="https://www.section.io/engineering-education/boosting-algorithms-python/#bagging-vs-boosting">Boosting Algorithms in Python</a></p>
<p><a href="https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/">GeeksforGeeks</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./visual.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Visualization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./advanced.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Advanced Topics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>